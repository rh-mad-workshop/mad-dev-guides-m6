= 4. Deploying a model - 15 minutes
:imagesdir: ../assets/images

After this overview of how a model is trained and how to use it, we're going to see how you can use RHODS to serve it, which will make it easier to use later on in our intelligent application.

RHODS integrates the https://docs.openvino.ai/latest/ovms_what_is_openvino_model_server.html[Intel's OpenVino Model Server^] runtime, a high-performance system for serving models, optimized for deployment on Intel architectures.

The model we are going to use is an **object detection model** that is able to isolate and recognize **T-shirts**, **bottles**, and **hats** in pictures. Although the process is globally the same one as what we have seen in the previous section, this model has already been trained as it takes a few hours with the help of a GPU to do it. If you want to know more about this training process, you can have a look https://github.com/rh-aiservices-bu/yolov5-transfer-learning[here^].

The resulting model has been saved in the https://onnx.ai/[ONNX^] format, an open standard for machine learning interoperability, which is one we can use with OpenVino and RHODS model serving. The model has been stored and is available for download in an AWS S3 bucket.

Let's serve this model!

== 4.1. Check the model

There are many ways and different tools available to interact with Object Storage. Here we are going to use the Python library named `boto3`.

* In the notebook we are going to use, you will need to enter a key to decrypt the credentials to access the model location. This key is:

[source,text]
----
Is_34AxyLht603Lh4z6UYztla79lQ3oz_os7U99JsKQ=
----

Please copy it, you will need it in a few seconds to replace the `replace_me` placeholder in the first cell of the notebook.

IMPORTANT: When you copy the key, don't forget to select the "=" sign at the end of the line.

(Don't worry, the bucket we are going to access is readonly, and those access/secret can only be used to read the public model file. So even the decrypted credentials are not sensitive at all.)

* In your project in JupyterLab, open the notebook `03_check_model` and follow the instructions to check the availability and the exact location of the model. Again, don't forget to run the cells!

image::check_model.png[]

* Once you are finished, you can come back to these instructions.

image::check_model-complete.png[]

== 4.2. Deploy the model in RHODS

With your model available on S3 storage, it is pretty easy to serve it.

* Switch back to the OpenShift Data Science dashboard. If you have closed it, you can access it https://rhods-dashboard-redhat-ods-applications.%SUBDOMAIN%[here^] or you can reopen it from Jupyter by clicking on **File->Hub Control Panel**:

image::hub_panel.png[]

* In the RHODS dashboard, still in your Data Science Project, create a Data connection. It will serve as a reference and configuration for access to the object bucket. In the **Data connections** section, select **Add data connection**:

image::add_data_connection.png[]

* Fill in the information, the select **Add data connection**:
    ** Name it `Coolstore`
    ** `Access Key` is the `Access key` you got in the previous step in the notebook.
    ** `Secret key` is the `Secret key` you got in the previous step in the notebook.
    ** `Endpoint` is `https://s3.amazonaws.com/` (default).
    ** `Region` is `us-east-1` (default).
    ** `Bucket` is `rh-mad-workshop-m6`.

No need to specify any connected workbench, this data connection will only be used by the model server.

image::data_connection_configuration.png[]

* You now have a data connection ready to use:

image::data_connection_ok.png[]

* Now, on the **Models and model servers** section select **Configure server**:

image::add_model_server.png[]

* Type *coolstore-modelserver* in the model server name. Select *OpenVINO Model Server* in Serving runtime. 
* Leave replicas to 1, size to Small. At this point, `don't` check **Make model available via an external route**. Then select **Add**:

image::model_server_configure.png[]

* We now have a model server available (but no model deployed yet). Select **Deploy model**:

image::model_server_available.png[]

* Give a name to your model, `coolstore`, select **onnx - 1** for the framework, select the Data location you created before for the Model location, and enter **coolstore-model** as the folder path for the model (without leading `/`), then select **Deploy**:

image::deploy_model_configuration.png[]

* The model is now deploying. You should click on the `1` on the `Deployed models` tab to see details:

image::deployed_models.png[]

* After some time, under the **Status** column, you should see a green tick to the right of you model, indicating it's ready for use!

image::model_ready.png[]

* Once it's ready, select the **Internal Service** link under the Inference endpoint column, and make note of the grpcURL value, we will need it later. It should be `grpc://modelmesh-serving.%USERID%-data-science-project:8033`

image::grpcurl.png[]

== 4.3. Test the model

Now that the model server is ready to receive requests, we can test it.

* In your project in JupyterLab, open the notebook `04_remote_inference` and follow the instructions to see how the model can be queried.

image::remote_inference.png[]

* Once you complete the notebook's instruction, you will end up with this result.

image::remote_inference_complete.png[]